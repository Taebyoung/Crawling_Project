{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DC Inside 만갤 개념글 db화\n",
    "- 주의 : 불법 번역 만화를 찾는 용도입니다. (현황 분석용)\n",
    "- DCinsice robots 정책\n",
    "\n",
    "```\n",
    "User-agent: *\n",
    "Disallow: /\n",
    "\n",
    "# Ads\n",
    "User-agent: grapeshot\n",
    "Allow: /  \n",
    "\n",
    "# Search\n",
    "User-agent: Googlebot\n",
    "Allow : / \n",
    "Crawl-delay: 60\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: scrapy.cfg already exists in C:\\Users\\yoon hwa\\Documents\\dev\\Crawling_Project\\comics\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject comics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더 PATH의 목록입니다.\n",
      "볼륨 일련 번호는 A876-8D28입니다.\n",
      "C:\\USERS\\YOON HWA\\DOCUMENTS\\DEV\\CRAWLING_PROJECT\\COMICS\n",
      "└─comics\n",
      "    ├─spiders\n",
      "    │  └─__pycache__\n",
      "    └─__pycache__\n"
     ]
    }
   ],
   "source": [
    "!tree comics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list + contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. xpath 추출하기\n",
    "- dc inside는 fake_useragent가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "from scrapy.http import TextResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fake_useragent import UserAgent\n",
    "UserAgent().chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 1\n",
    "headers = {\n",
    "    \"user-agent\":UserAgent().chrome\n",
    "}\n",
    "req = requests.get(url=\"https://gall.dcinside.com/board/lists/?id=comic_new2&page={}&exception_mode=recommend\".format(page), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Response [200]>,\n",
       " <200 https://gall.dcinside.com/board/lists/?id=comic_new2&page=1&exception_mode=recommend>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links\n",
    "links = response.xpath('//*[@class=\"ub-content us-post\"]/td[2]/a[1]/@href').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://gall.dcinside.com/board/view/?id=comic_new2&no=1085660&exception_mode=recommend&page=1'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = response.urljoin(links[0])\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(url=link, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200 https://gall.dcinside.com/board/view/?id=comic_new2&no=1085660&exception_mode=recommend&page=1>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'번역)LV1 마왕과 원룸 용사 9-2화(빛나랑 골덕 작가)'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# title\n",
    "title = response.xpath('//*[@class=\"gallview_head clear ub-content\"]/h3/span[2]/text()').extract_first()\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4173'"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# views\n",
    "views = response.xpath('//*[@class=\"fr\"]/span[1]/text()').extract_first()\n",
    "views[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019.12.10 23:58:07'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# date\n",
    "date = response.xpath('//*[@class=\"fl\"]/span[2]/text()').extract_first()\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'222'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recommend\n",
    "recommend = response.xpath('//*[@class=\"gall_reply_num\"]/text()').extract_first()\n",
    "recommend[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이미지 개수\n",
    "imgs = response.xpath('//*[@style=\"overflow:hidden;\"]/a/@href').extract()\n",
    "len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://2.bp.blogspot.com/-n-Pk3MTs1oE/Xe-xtRtpyJI/AAAAAAAAFO8/lVeiuVghkrMeSm5sNLCEL5c5Zhz6bvdVQCLcBGAsYHQ/s5000/9-015.png'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫번째 이미지 링크\n",
    "imgs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. items.py 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting comics/comics/items.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile comics/comics/items.py\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class ComicsItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    date = scrapy.Field()\n",
    "    views = scrapy.Field()\n",
    "    recommend = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    img_count = scrapy.Field()\n",
    "    img_link = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. spider 만들기\n",
    "- error : 이미지가 없는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting comics/comics/spiders/spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile comics/comics/spiders/spider.py\n",
    "import scrapy\n",
    "from comics.items import ComicsItem\n",
    "\n",
    "class ComicsSpider(scrapy.Spider):\n",
    "    name = \"Comics\"\n",
    "    custom_settings = {\n",
    "        'DOWNLOADER_MIDDLEWARES' : {\n",
    "            'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n",
    "            'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, pages=1, **kwargs):\n",
    "        self.start_url = \"https://gall.dcinside.com/board/lists/?id=comic_new2&page={}&exception_mode=recommend\".format(pages)\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def start_requests(self):\n",
    "        url = self.start_url\n",
    "        yield scrapy.Request(url, callback=self.parse)\n",
    "        \n",
    "    def parse(self, response):\n",
    "        links = response.xpath('//*[@class=\"ub-content us-post\"]/td[2]/a[1]/@href').extract()\n",
    "        links = list(map(response.urljoin, links))\n",
    "        for link in links:\n",
    "            yield scrapy.Request(link, callback=self.page_parse)\n",
    "    \n",
    "    def page_parse(self, response):\n",
    "        item = ComicsItem()\n",
    "        item[\"title\"] = response.xpath('//*[@class=\"gallview_head clear ub-content\"]/h3/span[2]/text()').extract_first()\n",
    "        item[\"date\"] = response.xpath('//*[@class=\"fl\"]/span[2]/text()').extract_first()\n",
    "        item[\"views\"] = response.xpath('//*[@class=\"fr\"]/span[1]/text()').extract_first()[3:]\n",
    "        item[\"recommend\"] = response.xpath('//*[@class=\"gall_reply_num\"]/text()').extract_first()[3:]\n",
    "        item[\"link\"] = response.url\n",
    "        try:\n",
    "            item[\"img_count\"] = len(response.xpath('//*[@style=\"overflow:hidden;\"]/a/@href').extract())\n",
    "            item[\"img_link\"] = response.xpath('//*[@style=\"overflow:hidden;\"]/a/@href').extract()[0]\n",
    "        except:\n",
    "            item[\"img_count\"] = \"0\"\n",
    "            item[\"img_link\"] = \"0\"\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. pipeline for mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting comics/comics/mongodb.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile comics/comics/mongodb.py\n",
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient('mongodb://15.165.28.220:27017/') # mongodb ip 필요\n",
    "db = client.comics\n",
    "collection = db.bestcomiecs\n",
    "# mongodb 모듈 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting comics/comics/pipelines.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile comics/comics/pipelines.py\n",
    "from .mongodb import collection\n",
    "\n",
    "class ComicsPipeline(object):\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        \n",
    "        data = {\n",
    "            \"title\": item[\"title\"], \n",
    "            \"date\": item[\"date\"],\n",
    "            \"views\": item[\"views\"], \n",
    "            \"recommend\": item[\"recommend\"],\n",
    "            \"link\": item[\"link\"],\n",
    "            \"img_count\": item[\"img_count\"],\n",
    "            \"img_link\": item[\"img_link\"],\n",
    "               }\n",
    "        collection.insert(data)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. setting.py 변경\n",
    "- ROBOTSTXT_OBEY = False\n",
    "- ITEM_PIPELINES = 300\n",
    "- DOWNLOAD_DELAY = 60 -> 1 : 최대 속도로 조정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting comics/comics/settings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile comics/comics/settings.py\n",
    "\n",
    "# Scrapy settings for comics project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'comics'\n",
    "\n",
    "SPIDER_MODULES = ['comics.spiders']\n",
    "NEWSPIDER_MODULE = 'comics.spiders'\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "#USER_AGENT = 'comics (+http://www.yourdomain.com)'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "#CONCURRENT_REQUESTS = 32\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "## DOWNLOAD_DELAY = 60\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "#COOKIES_ENABLED = False\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "#DEFAULT_REQUEST_HEADERS = {\n",
    "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#   'Accept-Language': 'en',\n",
    "#}\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'comics.middlewares.ComicsSpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    'comics.middlewares.ComicsDownloaderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "ITEM_PIPELINES = {\n",
    "    'comics.pipelines.ComicsPipeline': 300,\n",
    "}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = 'httpcache'\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. run 실행 파일 생성\n",
    "- -o 파일이름.csv\n",
    "- default = -a page=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "scrapy crawl Comics -o dc_comics_191211.csv -a page=25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
