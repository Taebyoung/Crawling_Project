{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DC Inside 만갤 개념글 다운로드\n",
    "---\n",
    "\n",
    "> 주의 : 국내에 출시되지 않은 만화책의 공유도 불법입니다.\n",
    "\n",
    "\n",
    "- DCinsice robots 정책\n",
    "    - 모든 유저의 크롤링 금지합니다.\n",
    "    - 빠른 크롤링은 IP 차단됩니다.\n",
    "\n",
    "```\n",
    "User-agent: *\n",
    "Disallow: /\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: scrapy.cfg already exists in C:\\Users\\yoon hwa\\Documents\\dev\\Crawling_Project\\comics\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject comics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더 PATH의 목록입니다.\n",
      "볼륨 일련 번호는 A876-8D28입니다.\n",
      "C:\\USERS\\YOON HWA\\DOCUMENTS\\DEV\\CRAWLING_PROJECT\\COMICS\n",
      "├─comics\n",
      "│  ├─spiders\n",
      "│  │  └─__pycache__\n",
      "│  └─__pycache__\n",
      "└─images\n",
      "    └─full\n"
     ]
    }
   ],
   "source": [
    "!tree comics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list + contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. items.py 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting comics/comics/items.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile comics/comics/items.py\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class ComicsItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # mongo-db\n",
    "    title = scrapy.Field()\n",
    "    date = scrapy.Field()\n",
    "    views = scrapy.Field()\n",
    "    recommend = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    img_count = scrapy.Field()\n",
    "    img_link = scrapy.Field()\n",
    "    \n",
    "    # downloads\n",
    "    image_urls = scrapy.Field()\n",
    "    images = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. spider 만들기\n",
    "- time:sleep(1) 로 크롤링 차단 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting comics/comics/spiders/spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile comics/comics/spiders/spider.py\n",
    "import scrapy\n",
    "import time\n",
    "from comics.items import ComicsItem\n",
    "from scrapy.pipelines.images import ImagesPipeline\n",
    "\n",
    "class ComicsSpider(scrapy.Spider):\n",
    "    name = \"Comics\"\n",
    "    custom_settings = {\n",
    "        'DOWNLOADER_MIDDLEWARES' : {\n",
    "            'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n",
    "            'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, pages=1, **kwargs):\n",
    "        self.start_url = \"https://gall.dcinside.com/board/lists/?id=comic_new2&page={}&exception_mode=recommend\".format(pages)\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def start_requests(self):\n",
    "        url = self.start_url\n",
    "        yield scrapy.Request(url, callback=self.parse)\n",
    "        \n",
    "    def parse(self, response):\n",
    "        links = response.xpath('//*[@class=\"ub-content us-post\"]/td[2]/a[1]/@href').extract()\n",
    "        links = list(map(response.urljoin, links))\n",
    "        for link in links:\n",
    "            yield scrapy.Request(link, callback=self.page_parse)\n",
    "    \n",
    "    def page_parse(self, response):\n",
    "        item = ComicsItem()\n",
    "        #title = response.xpath('//*[@class=\"gallview_head clear ub-content\"]/h3/span[2]/text()').extract_first()\n",
    "        item[\"title\"] = response.xpath('//*[@class=\"gallview_head clear ub-content\"]/h3/span[2]/text()').extract_first()\n",
    "        item[\"date\"] = response.xpath('//*[@class=\"gall_date\"]/text()').extract_first()\n",
    "        item[\"views\"] = response.xpath('//*[@class=\"fr\"]/span[1]/text()').extract_first()[3:]\n",
    "        item[\"recommend\"] = response.xpath('//*[@class=\"gall_reply_num\"]/text()').extract_first()[3:]\n",
    "        item[\"link\"] = response.url\n",
    "            \n",
    "        try:\n",
    "            try:\n",
    "                # for DB\n",
    "                item[\"img_link\"] = response.xpath('//*[@style=\"overflow:hidden;\"]/p/img/@src').extract()[0]\n",
    "                item[\"img_count\"] = len(response.xpath('//*[@style=\"overflow:hidden;\"]/p/img/@src').extract())\n",
    "                item[\"image_urls\"] = response.xpath('//*[@style=\"overflow:hidden;\"]/p/img/@src').extract()\n",
    "                               \n",
    "                #for Images\n",
    "                #images = []\n",
    "                #img_urls = response.xpath('//*[@style=\"overflow:hidden;\"]/p/img/@src').extract()\n",
    "                #img_count = len(response.xpath('//*[@style=\"overflow:hidden;\"]/p/img/@src').extract())\n",
    "                \n",
    "                #img_names = [ title + \"_\" + str(n) for n in range(img_count)]\n",
    "                #for image_url, image_name in zip(img_urls, img_names):\n",
    "                #    images.append({'url': image_url, 'name': image_name})\n",
    "                #\n",
    "                #item[\"image_urls\"] = images\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "            try:\n",
    "                # for DB\n",
    "                item[\"img_link\"] = response.xpath('//*[@style=\"overflow:hidden;\"]/a/@href').extract()[0]\n",
    "                item[\"img_count\"] = len(response.xpath('//*[@style=\"overflow:hidden;\"]/a/@href').extract())\n",
    "                item[\"image_urls\"] = response.xpath('//*[@style=\"overflow:hidden;\"]/a/@href').extract()\n",
    "                               \n",
    "                # for Images\n",
    "                #images = []\n",
    "                #img_urls = response.xpath('//*[@style=\"overflow:hidden;\"]/a/@href').extract()\n",
    "                #img_count = len(response.xpath('//*[@style=\"overflow:hidden;\"]/a/@href').extract())\n",
    "                #\n",
    "                #img_names = [ title + \"_\" + str(n) for n in range(img_count)]\n",
    "                #for image_url, image_name in zip(img_urls, img_names):\n",
    "                #    images.append({'url': image_url, 'name': image_name})\n",
    "                #\n",
    "                #item[\"image_urls\"] = images\n",
    "            except:\n",
    "                pass\n",
    "        except:\n",
    "            item[\"img_count\"] = \"0\"\n",
    "            item[\"img_link\"] = \"\"\n",
    "        \n",
    "        time.sleep(1) # 게시물 check 딜레이\n",
    "        \n",
    "        yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. pipeline for mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting comics/comics/mongodb.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile comics/comics/mongodb.py\n",
    "import pymongo\n",
    "\n",
    "# mongodb 모듈 생성\n",
    "client = pymongo.MongoClient('mongodb://15.165.28.220:27017/') # mongodb ip 필요\n",
    "db = client.comics\n",
    "collection = db.dcinsidecomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting comics/comics/pipelines.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile comics/comics/pipelines.py\n",
    "from .mongodb import collection\n",
    "import scrapy\n",
    "from scrapy.pipelines.images import ImagesPipeline\n",
    "from scrapy.exceptions import DropItem\n",
    "\n",
    "\n",
    "# mongo_db 설정\n",
    "class ComicsPipeline(object):\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        \n",
    "        data = {\n",
    "            \"title\": item[\"title\"], \n",
    "            \"date\": item[\"date\"],\n",
    "            \"views\": item[\"views\"], \n",
    "            \"recommend\": item[\"recommend\"],\n",
    "            \"link\": item[\"link\"],\n",
    "            \"img_count\": item[\"img_count\"],\n",
    "            \"img_link\": item[\"img_link\"],\n",
    "               }\n",
    "        collection.insert(data)\n",
    "        return item\n",
    "\n",
    "# image download\n",
    "class MyImagesPipeline(ImagesPipeline):\n",
    "\n",
    "    def get_media_requests(self, item, info):\n",
    "        for image_url in item['image_urls']:\n",
    "            yield scrapy.Request(url=image_url, headers={'Referer':item['link']})\n",
    "\n",
    "    def item_completed(self, results, item, info):\n",
    "        image_paths = [x['path'] for ok, x in results if ok]\n",
    "        if not image_paths:\n",
    "            raise DropItem(\"Item contains no images\")\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. setting.py 변경\n",
    "- ROBOTSTXT_OBEY = False\n",
    "- ITEM_PIPELINES = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting comics/comics/settings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile comics/comics/settings.py\n",
    "\n",
    "# Scrapy settings for comics project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'comics'\n",
    "\n",
    "SPIDER_MODULES = ['comics.spiders']\n",
    "NEWSPIDER_MODULE = 'comics.spiders'\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "#USER_AGENT = 'comics (+http://www.yourdomain.com)'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "\n",
    "# Configure item pipelines\n",
    "ITEM_PIPELINES = {\n",
    "    'comics.pipelines.ComicsPipeline': 300,\n",
    "    'scrapy.pipelines.images.ImagesPipeline': 1\n",
    "}\n",
    "\n",
    "# Configure download\n",
    "IMAGES_STORE = 'images'\n",
    "\n",
    "# 다운로드 딜레이\n",
    "DOWNLOAD_DELAY = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. run 실행 파일 생성\n",
    "- -o 파일이름.csv\n",
    "- default = -a page=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "cd ./comics\n",
    "scrapy crawl Comics -a page=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
